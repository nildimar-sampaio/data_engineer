{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1qo8eFJaAeh4UCfpXWAAxE3TPVVEQ2oqc",
      "authorship_tag": "ABX9TyNeBDiZplkT8FQuu86SjDjI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nildimar-sampaio/data_engineer/blob/main/Data_Engineer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Engineer Case\n",
        "**Medallion Architecture**\n",
        "\n",
        "Using Python + Pandas to manipulate the data.\n",
        "Datasets stored into Google Drive"
      ],
      "metadata": {
        "id": "fG7h7YfrF7VJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c2Z2jVRT7xaw",
        "outputId": "362dd239-bb7c-49a6-c6d2-1d77425ed3ab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "#Google drive integration and mount for dataset access\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Bronze Phase\n",
        "Checking dataset and understanding the data.\n",
        "\n",
        "Found Multiple CSVs in multiple folders.\n",
        "\n",
        "Automatized the process on data visualize using a routine to print the\n",
        "dataframe of each CSV file from this folder."
      ],
      "metadata": {
        "id": "kknqshH6HYnL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Automatize on reading all CSV files from the folder 2025-01-21\n",
        "#Visualize the data\n",
        "\n",
        "import pandas as pd\n",
        "import glob\n",
        "\n",
        "path_folder = \"/content/drive/MyDrive/dataset/20250121/\"\n",
        "\n",
        "files_csv = glob.glob(f\"{path_folder}/*.csv\")\n",
        "\n",
        "for eachfile in files_csv: #loop for reading each file and printing dataframe\n",
        "    print(f\"Reading file: {eachfile}\")\n",
        "    df = pd.read_csv(eachfile)\n",
        "    print(df)\n",
        "    print(\"\\n\" + \"=*\"*40 + \"\\n\")  # print file separator text\n"
      ],
      "metadata": {
        "id": "Uj1AWnk1MxG_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Automatize on reading all CSV files from the folder 2025-01-22\n",
        "#Visualize the data\n",
        "\n",
        "import pandas as pd\n",
        "import glob\n",
        "\n",
        "path_folder = \"/content/drive/MyDrive/dataset/20250122/\"\n",
        "\n",
        "files_csv = glob.glob(f\"{path_folder}/*.csv\")\n",
        "\n",
        "for eachfile in files_csv: #loop for reading each file and printing dataframe\n",
        "    print(f\"Reading file: {eachfile}\")\n",
        "    df = pd.read_csv(eachfile)\n",
        "    print(df)\n",
        "    print(\"\\n\" + \"=*\"*40 + \"\\n\")  # print file separator text"
      ],
      "metadata": {
        "id": "4u6yjgbvHy-i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Automatize on reading all CSV files from the folder 2025-01-23\n",
        "#Visualize the data\n",
        "\n",
        "import pandas as pd\n",
        "import glob\n",
        "\n",
        "path_folder = \"/content/drive/MyDrive/dataset/20250123/\"\n",
        "\n",
        "files_csv = glob.glob(f\"{path_folder}/*.csv\")\n",
        "\n",
        "for eachfile in files_csv: #loop for reading each file and printing dataframe\n",
        "    print(f\"Reading file: {eachfile}\")\n",
        "    df = pd.read_csv(eachfile)\n",
        "    print(df)\n",
        "    print(\"\\n\" + \"=*\"*40 + \"\\n\")  # print file separator text"
      ],
      "metadata": {
        "id": "X8DRetZ7Tq4c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Automatize on reading all CSV files from the folder 2025-01-24\n",
        "#Visualize the data\n",
        "\n",
        "import pandas as pd\n",
        "import glob\n",
        "\n",
        "path_folder = \"/content/drive/MyDrive/dataset/20250124/\"\n",
        "\n",
        "files_csv = glob.glob(f\"{path_folder}/*.csv\")\n",
        "\n",
        "for eachfile in files_csv: #loop for reading each file and printing dataframe\n",
        "    print(f\"Reading file: {eachfile}\")\n",
        "    df = pd.read_csv(eachfile)\n",
        "    print(df)\n",
        "    print(\"\\n\" + \"=*\"*40 + \"\\n\")  # print file separator text"
      ],
      "metadata": {
        "id": "iuzsfpMbTrRv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Automatize on reading all CSV files from the folder 2025-01-25\n",
        "#Visualize the data\n",
        "\n",
        "import pandas as pd\n",
        "import glob\n",
        "\n",
        "path_folder = \"/content/drive/MyDrive/dataset/20250125/\"\n",
        "\n",
        "files_csv = glob.glob(f\"{path_folder}/*.csv\")\n",
        "\n",
        "for eachfile in files_csv: #loop for reading each file and printing dataframe\n",
        "    print(f\"Reading file: {eachfile}\")\n",
        "    df = pd.read_csv(eachfile)\n",
        "    print(df)\n",
        "    print(\"\\n\" + \"=*\"*40 + \"\\n\")  # print file separator text"
      ],
      "metadata": {
        "id": "Ed5Ao8_2TrmK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finishing the Bronze phase visualizing and understanding the data.\n",
        "Looks the data is related to the Nourish(Flavors) Businness Unit.\n",
        "I can see data set of ingredients, providers, recipes, and sales transactions."
      ],
      "metadata": {
        "id": "94mlg06_i4EZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Silver Phase\n",
        "Checking data integrity, consistency for these datasets.\n",
        "Again using automation, but this time I put all folders into an array to increase the automation level."
      ],
      "metadata": {
        "id": "dAevR4sUlglJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import glob\n",
        "\n",
        "#Putting folder paths into an Array\n",
        "\n",
        "path_folder0 = \"/content/drive/MyDrive/dataset/20250121/\"\n",
        "path_folder1 = \"/content/drive/MyDrive/dataset/20250122/\"\n",
        "path_folder2 = \"/content/drive/MyDrive/dataset/20250123/\"\n",
        "path_folder3 = \"/content/drive/MyDrive/dataset/20250124/\"\n",
        "path_folder4 = \"/content/drive/MyDrive/dataset/20250125/\"\n",
        "\n",
        "# create array\n",
        "folder_paths = [path_folder0, path_folder1, path_folder2, path_folder3, path_folder4]\n",
        "\n",
        "#loop reading each folder\n",
        "for path_folder in folder_paths:\n",
        "  files_csv = glob.glob(f\"{path_folder}/*.csv\")\n",
        "\n",
        "  for eachfile in files_csv: #loop for reading each file\n",
        "    print(f\"Reading file: {eachfile}\")\n",
        "    df = pd.read_csv(eachfile)\n",
        "    print('\\nHave duplicates?')\n",
        "    print(df.duplicated()) #check for duplicates\n",
        "    print('\\nHave null values?')\n",
        "    print(df.isna()) #check for nulll values\n",
        "    print(\"\\n\" + \"=*\"*40 + \"\\n\")  # print file separator text\n",
        "\n"
      ],
      "metadata": {
        "id": "qK9HUj52lkpv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now create relationships"
      ],
      "metadata": {
        "id": "ol-TWUnfLGsj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Join dataframe Sales Transactions and getting data from Flavours dataframe from Folder 21\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "#dataframe for Flavour information\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/dataset/20250121/Flavours_20240505_1.csv\")\n",
        "\n",
        "#dataframes for Sales Transactions\n",
        "df1 = pd.read_csv(\"/content/drive/MyDrive/dataset/20250121/SalesTransactions_20240505_1.csv\")\n",
        "df2 = pd.read_csv(\"/content/drive/MyDrive/dataset/20250121/SalesTransactions_20240505_2.csv\")\n",
        "df3 = pd.read_csv(\"/content/drive/MyDrive/dataset/20250122/SalesTransactions_20240506_1.csv\")\n",
        "df4 = pd.read_csv(\"/content/drive/MyDrive/dataset/20250122/SalesTransactions_20240506_2.csv\")\n",
        "\n",
        "df_full = pd.concat([df1, df2, df3, df4]) #concatenate all Sales Transactions\n",
        "\n",
        "relationship = pd.merge(df, df_full, how = 'inner', on = 'flavour_id') #relationship Sales Transactiuon with Flavour information\n",
        "relationship #print list"
      ],
      "metadata": {
        "id": "p_o7-QhoLKnU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Relationships now with Recipes dataframe\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "#dataframe for Flavour information\n",
        "df_flavour = pd.read_csv(\"/content/drive/MyDrive/dataset/20250121/Flavours_20240505_1.csv\")\n",
        "\n",
        "#dataframe for Ingredients information\n",
        "df_ingredient = pd.read_csv(\"/content/drive/MyDrive/dataset/20250122/Ingredients_20240506_1.csv\")\n",
        "\n",
        "#dataframes for Recipes\n",
        "df1 = pd.read_csv(\"/content/drive/MyDrive/dataset/20250122/Recipes_20240507_1.csv\")\n",
        "df2 = pd.read_csv(\"/content/drive/MyDrive/dataset/20250122/Recipes_20240507_2.csv\")\n",
        "df3 = pd.read_csv(\"/content/drive/MyDrive/dataset/20250122/Recipes_20240507_3.csv\")\n",
        "df4 = pd.read_csv(\"/content/drive/MyDrive/dataset/20250122/Recipes_20240510_1.csv\")\n",
        "df5 = pd.read_csv(\"/content/drive/MyDrive/dataset/20250122/Recipes_20240510_2.csv\")\n",
        "\n",
        "df_full = pd.concat([df1, df2, df3, df4, df5]) #concatenate all Recipes\n",
        "\n",
        "relationship_flavour = pd.merge(df_flavour, df_full, how = 'inner', on = 'flavour_id') #relationship recipes with flavour information\n",
        "relationship_ingredient = pd.merge(df_ingredient, relationship_flavour, how = 'inner', on = 'ingredient_id') #relationship above plus ingredient information\n",
        "relationship_ingredient #print list"
      ],
      "metadata": {
        "id": "pU2q60LFdRDy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Gold Phase\n",
        "Here I will show relevant information for Businness decisions\n",
        "\n",
        "\n",
        "1.   List 10 Lowest stock Materials\n",
        "2.   List top 50 grossing transactions\n",
        "3. Plot a Graph of these 50 top grossing customers and Liters sold\n",
        "\n"
      ],
      "metadata": {
        "id": "yxj9byULhjgS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Show Low stock materials - 10 lowest\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/dataset/20250125/Stocks_20240505_1.csv\") # dataframe for Stock Information\n",
        "df_flavour = pd.read_csv(\"/content/drive/MyDrive/dataset/20250121/Flavours_20240505_1.csv\") #dataframe for Flavour information\n",
        "\n",
        "low_stock = pd.merge(df, df_flavour, how = 'inner', on = 'flavour_id') #merge with Flavour information\n",
        "\n",
        "low_list = low_stock.sort_values(by='quantity_liters', ascending=True).head(10) #sort the dataframe showing the lowest 10 stock\n",
        "\n",
        "#print 10 lowest with flavour information\n",
        "low_list\n"
      ],
      "metadata": {
        "id": "sXAB72dIhxQh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#show top 50 grossing transactions\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#dataframe for Customers information\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/dataset/20250124/Customers_20240508_1.csv\")\n",
        "\n",
        "#dataframes for Sales Transactions\n",
        "df1 = pd.read_csv(\"/content/drive/MyDrive/dataset/20250121/SalesTransactions_20240505_1.csv\")\n",
        "df2 = pd.read_csv(\"/content/drive/MyDrive/dataset/20250121/SalesTransactions_20240505_2.csv\")\n",
        "df3 = pd.read_csv(\"/content/drive/MyDrive/dataset/20250122/SalesTransactions_20240506_1.csv\")\n",
        "df4 = pd.read_csv(\"/content/drive/MyDrive/dataset/20250122/SalesTransactions_20240506_2.csv\")\n",
        "\n",
        "df_full = pd.concat([df1, df2, df3, df4]) #concatenate all Sales Transactions\n",
        "\n",
        "relationship = pd.merge(df, df_full, how = 'inner', on = 'customer_id') #relationship Sales Transaction with Customer Information\n",
        "\n",
        "top_grossing = relationship.sort_values(by='amount_dollar', ascending=False).head(50) # select top 50\n",
        "\n",
        "top_grossing_print = top_grossing[[\"name\", \"amount_dollar\"]] #select only Name and amount_dollar for printing\n",
        "\n",
        "top_grossing_print"
      ],
      "metadata": {
        "id": "d8YLeqSokc4p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Plot a Graph with the Top 50 grossing Customer and the amount of Liters sold\n",
        "\n",
        "plt.figure(figsize=(12,5))\n",
        "plt.bar(top_grossing[\"name\"], top_grossing[\"quantity_liters\"], color=\"blue\", label=\"Liters Sold\")\n",
        "#add extra information to the graph and rotate axis x\n",
        "plt.title(\"Liters Sold\")\n",
        "plt.xlabel(\"Customer\")\n",
        "plt.ylabel(\"Liters\")\n",
        "plt.xticks(rotation=90)\n",
        "plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.5)\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "FY6_0mGQL0sT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Analysis finished.\n",
        "All 3 Medallion performed.\n",
        "\n",
        "Conclusion: As confirmed in the Bronze Phase, these data is related to Flavors (Nourish) Businness Unit.\n",
        "Transactions of sales were identified and top grossing ones.\n",
        "An attention to the low stock for some materials can lead to delays to customer."
      ],
      "metadata": {
        "id": "ha3UJ-yhUI5h"
      }
    }
  ]
}